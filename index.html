<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Learning Human-Perceived Fakeness in Generated Videos with Multimodal LLMs">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Visual Generation, Vision and Language Reasoning, Vision Language Model, LLM, VLM, Visual Chain of Thought, Visual Prompting">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DeeptraceReward</title>
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/dataTables.bulma.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  </head>
  <body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zeyofu.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://zeyofu.github.io/blink/">
            BLINK
          </a>
          <a class="navbar-item" href="https://visualsketchpad.github.io/">
            Visual Sketchpad
          </a>
          <a class="navbar-item" href="https://muirbench.github.io/">
            MuirBench
          </a>
          <a class="navbar-item" href="https://zeyofu.github.io/CommonsenseT2I/">
            Commonsense-T2I
          </a>
          <a class="navbar-item" href="https://zeyofu.github.io/ReFocus/">
            ReFocus
          </a>
        </div>
      </div>
    </div>

  </div>
  </nav>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF (NeurIPS 2023)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><img src="static/images/icon.png" width="100" /><font color="#96482c"> DeeptraceReward</font>: Learning Human-Perceived Fakeness in Generated Videos with Multimodal LLMs</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://zeyofu.github.io/" target="_blank"><font color="#B082C9"><b>Xingyu Fu</b></font></a><sup class="princeton-sup">𝒑</sup><sup class="penn-sup">𝒑</sup><a href="mailto:xingyufu@princeton.edu" style="margin-left: 5px;"><i class="fas fa-envelope" style="color: #B082C9;"></i></a>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://liusiyi641.github.io/" target="_blank"><font color="#B082C9"><b>Siyi Liu</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://yinoxu.github.io/" target="_blank"><font color="#B082C9"><b>Yinuo Xu</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://lupantech.github.io/" target="_blank"><font color="#B082C9"><b>Pan Lu</b></font></a><sup class="stanford-sup">𝒔</sup>&emsp;
                </span>
                <br>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/violahu930/" target="_blank"><font color="#B082C9"><b>Guangqiuse Hu</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/tianbo-yang-815668247/" target="_blank"><font color="#B082C9"><b>Tianbo Yang</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/taran-anantasagar/" target="_blank"><font color="#B082C9"><b>Taran Anantasagar</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/chrshen/" target="_blank"><font color="#B082C9"><b>Christopher Shen</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <br>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/yikai-mao/" target="_blank"><font color="#B082C9"><b>Yikai Mao</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/yuanzhe-liu-upenn-ds/" target="_blank"><font color="#B082C9"><b>Yuanzhe Liu</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/keyush-shah-3a1b32184/" target="_blank"><font color="#B082C9"><b>Keyush Shah</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/chung-un-lee-3a2035131/" target="_blank"><font color="#B082C9"><b>Chung Un Lee</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <br>
                <span class="author-block">
                  <a href="https://yejinc.github.io/" target="_blank"><font color="#B082C9"><b>Yejin Choi</b></font></a><sup class="stanford-sup">𝒔</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.james-zou.com/" target="_blank"><font color="#B082C9"><b>James Zou</b></font></a><sup class="stanford-sup">𝒔</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.cis.upenn.edu/~danroth/" target="_blank"><font color="#B082C9"><b>Dan Roth</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.cis.upenn.edu/~ccb/" target="_blank"><font color="#B082C9"><b>Chris Callison-Burch</b></font></a><sup class="penn-sup">𝒑</sup>&emsp;
                </span>
                </div>
                <br>
                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <sup class="princeton-sup">𝒑</sup><img src="static/images/princeton.png" width="170" />&emsp;
                    <sup class="penn-sup">𝒑</sup><img src="static/images/penn.jpg" width="120" />&emsp;
                    <sup class="stanford-sup">𝒔</sup><img src="static/images/stanford.png" width="130" /> &emsp;
                    <!-- <sup>*</sup>Equal Contribution&emsp;
                    <sup>†</sup>Equal Advising -->
                  </span>
                  <br>
                  <!-- <span class="author-block">
                    <h1 class="title is-4"><font color="#B03A2E"><b>ICML 2025</b></font></h1>
                  </span> -->
                </div>

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2501.05452" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://github.com/deeptracereward/deeptracereward" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                    </span>
  
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/DeepTraceReward/RewardData" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        🤗
                      </span>
                      <span>Training Dataset</span>
                    </a>
                  </span>

                <span class="link-block">
                  <a href="https://huggingface.co/DeepTraceReward/RewardModel" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-globe"></i>
                  </span>
                  <span>Model Checkpoint</span>
                  </a>
                </span>

                <!-- <span class="link-block">
                  <a href="https://twitter.com/XingyuFu2/status/1781368539213082683" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
                </span> -->
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">What is DeeptraceReward?</h2>
      <h2 class="subtitle has-text-justified">
        <span style="font-weight:bold;">DeeptraceReward </span> answers whether human can explicitly identify deepfake traces, <em>i.e.</em>, visual artifacts that reveal a video as machine generated, within a generated video for the first time.</h2>
      <img src="static/images/main_figure.png" height="100%"/>
      <h2 class="subtitle has-text-justified">
        <!-- XXX <span style="font-weight:bold;">XXX </span> XXX <span style="font-weight:bold;">XXX </span> XXX</h2> -->
        <h2 class="subtitle has-text-justified">
          Can humans distinguish AI-generated videos from natural one and provide grounded reasons for their judgments?  </h2>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">DeeptraceReward</span>.</h2> -->
      <!-- <h2 class="hero-body has-text-centered">
        <br>
        Example tasks in <span style="font-weight:bold;">DeeptraceReward</span>. The answers of the examples: relative depth: B; jigsaw: A; multi-view reasoning: right; visual correspondence: A; semantic correspondence: C; forensics detection: final image; IQ test: D; visual similarity: upper one; functional correspondence: A; relative reflectance: they are about the same.
      </h2> -->
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Video comparison section -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Human-Perceived Fake Traces</h2>
      <div class="columns is-centered">
        <div class="column is-half">
          <video autoplay muted loop controls style="width: 105%;">
            <source src="static/videos/sora.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <h3 class="subtitle has-text-centered">Original Video</h3>
        </div>
        <div class="column is-half">
          <video autoplay muted loop controls style="width: 95%;">
            <source src="static/videos/annotated_sora.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <h3 class="subtitle has-text-centered">Annotated Video</h3>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video comparison section -->

<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Brief video introduction about <span style="font-weight:bold;">DeeptraceReward Benchmark.</span>. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <strong>Can human identify AI-generated (fake) videos and provide grounded reasons?</strong>
            While video generation models have advanced rapidly, a critical dimension –
            whether human can detect deepfake traces within a generated video, i.e., spa-
            tiotemporal grounded visual artifacts that reveal a video as machine generated –
            has been largely overlooked.
          </p>
          
          <p>
            We introduce <span style="background-color: #f0f8ff; padding: 2px 4px; border-radius: 3px;"><strong>DEEPTRACEREWARD</strong></span>, the first fine-
            grained, spatially and temporally aware benchmark that annotates human-perceived
            fake traces for video generation reward. The dataset comprises <span style="background-color: #fff2cc; padding: 2px 4px; border-radius: 3px;"><strong>4.3K detailed
            annotations across 3.3K high-quality generated videos</strong></span>. Each annotation provides
            a natural-language explanation, pinpoints a bounding-box region containing the
            perceived trace, and marks precise onset and offset timestamps.
          </p>
          
          <p>
            We consolidate these annotations into <span style="background-color: #e8f5e8; padding: 2px 4px; border-radius: 3px;"><strong>9 major categories of deepfake traces</strong></span> that lead humans to
            identify a video as AI-generated, and train multimodal language models (LMs)
            as reward models to mimic human judgments and localizations. On DEEPTRAC-
            EREWARD, our <span style="background-color: #ffe6e6; padding: 2px 4px; border-radius: 3px;"><strong>7B reward model outperforms GPT-5 by 34.7%</strong></span> on average across
            fake clue identification, grounding, and explanation.
          </p>
          
          <p>
            Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially
            easier than fine-grained deepfake trace detection; within the latter, performance
            degrades from natural language explanations (easiest), to spatial grounding, to
            temporal labeling (hardest). By foregrounding human-perceived deepfake traces,
            DEEPTRACEREWARD provides a rigorous testbed and training signal for socially
            aware and trustworthy video generation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Human-Perceived Fakeness Examples</h2>
        <img src="static/images/example1.png" width="90%"/>
        <!-- <img src="static/images/example2.png" width="100%"/> -->
        <h2 class="content has-text-centered">
          <span style="font-size: 1.2em; font-weight: bold; color: #96482c;">
            What kind of videos should we collect?
          </span>
        <div class="content has-text-justified" style="margin-top: 1em;">
          <p>
            Two key criteria in our video collection process is to include only <b>high-quality</b> generated videos that <b>contain motion</b>. The first criterion is motivated by annotation challenges observed in low-quality videos generated by many open-source models, which tend to be ambiguous, extremely short (e.g., only 1 second), or entirely distorted across all frames -- making them unsuitable for fine-grained deepfake trace identification.
          </p>
          <p>
            The second criterion comes from our initial observations, that most retained videos after manual filtering depict dynamic scenes involving object or human movement. This bias is deliberate: artifact patterns such as unnatural trajectories, object distortions, and sudden blurring are far more likely to emerge in motion-rich scenarios than in static scenarios, which rarely exhibit consistent visual anomalies. These insights guide our video selection and annotation strategy, which prioritizes movement-centric contexts where fake clues are more prevalent.
          </p>
        </div>

        </h2>
        
      </div>
    </div>
  </div>
</section>


<!-- Paper Qualitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Data Collection</h2>
        <img src="static/images/collection.png" height="90%"/>
        <h2 class="content has-text-justified">
          DeeptraceReward data curation pipeline. Selected videos are uploaded to our annotation platform <a href="https://labelbox.com/?utm_source=google&utm_term=labelbox&utm_campaign=20490363302&utm_medium=paid-search&ad_group=152140922585&ad=708454775184&ntwk=g&tgt=kwd-444177053015&_bm=p&gad_source=1&gad_campaignid=20490363302&gbraid=0AAAAAC6g_7vg56NxXnf-WT66btnBkC3Xv&gclid=Cj0KCQjwxL7GBhDXARIsAGOcmIOHmXstuPK-vXMP_VEA19hQ2CZLGn3Sks_zWavvaxIDoOX-1VTRq9waAl_xEALw_wcB" target="_blank">LabelBox</a>, where experts provide fine-grained deepfake trace annotations with bounding boxes, textual explanations, and start / end timestamps.          
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Paper Analysis -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3"> <b>DeeptraceReward</b> Statistics </h2> <br>
    </div>
    <div class="columns is-centered">
      <div class="column is-five-sixths">
        <p>
          The dataset consists of <b>4,334 fake videos</b> generated by six state-of-the-art T2V models and <b>3,318 real videos</b> sourced from LLaVA-Video-178K (Zhang et al., 2024) for training. For both sources, we report: the number of videos, the proportion with human-written explanations, average resolution (mean height and width), average video length (seconds), and average length of annotated fake clues (based on start/end timestamps). The dataset features substantial diversity in both resolution and temporal duration across models.
        <br
        </p>  
        <br>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/statistics.png" width="90%"/> </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper Qualitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Experiment Results</h2>
        <img src="static/images/results.png" width="100%"/>
        <br>
        <h2 class="content has-text-justified">
          All baseline models achieve below 37% performance regardless of their sizes. The SOTA models GPT-5, GPT-4.1, and Gemini 2.5 Pro are the only ones to have an overall score over 30%. In contrast, our best 7B model based on VideoLLaMA 3 can easily surpass GPT-5 by 34.7%, and Gemini 2.5 Pro by 40.2%, reaching 70.2% after training on our high-quality dataset. Interestingly, we observe a consistent difficulty gradient: binary classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). <b>&uarr;</b> means higher is better, <b>&darr;</b> means lower is better.
        </h2>
        <br>
          <!-- <img src="static/images/category.png" width="90%"/> -->
      </div>
    </div>
  </div>
</section>



<!-- <section class="hero is-small">
  <div class="container is-max-desktop content">
    <br>
    <h2 class="title is-3">Related Work</h2>
    <ul>
      <li> <a href="https://visualsketchpad.github.io/">Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models</a></li>
      <li> <a href="https://prior.allenai.org/projects/visprog">Visual Programming for Compositional Visual Reasoning</a></li>
      <li> <a href="https://viper.cs.columbia.edu/">ViperGPT: Visual Inference via Python Execution for Reasoning</a></li>
      <li> <a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
      <li> <a href="https://whiteboard.cs.columbia.edu/">Whiteboard-of-Thought: Thinking Step-by-Step Across Modalities</a></li>
      <li> <a href="https://arxiv.org/abs/2404.19205">TableVQA-Bench: A Visual Question Answering Benchmark on Multiple Table Domains</a> </li>
      <li> <a href="https://arxiv.org/abs/2203.10244">ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning</a> </li>
      <li> <a href="https://charxiv.github.io/">CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs</a> </li>
      <li> <a href="https://huggingface.co/microsoft/Phi-3.5-vision-instruct">Phi-3.5-vision Model</a> </li>
      <li> <a href="https://github.com/deepcs233/Visual-CoT">Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning</a> </li>
      <li> <a href="https://microsoft.github.io/visualization-of-thought/#/">Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models</a> </li>
    </ul>
  </div>
</section> -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><bibtexcode>
        To Add Soon.
      </bibtexcode></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
  
    <script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.2.1/js/bootstrap.bundle.min.js"></script>
    <script src="./static/js/jquery.csv.min.js"></script>
    <script src="https://cdn.datatables.net/2.0.8/js/dataTables.min.js"></script>
    <script src="https://cdn.datatables.net/2.0.8/js/dataTables.bulma.min.js"></script>
    <script src="./static/js/csv_to_html_table.js"></script>
    <script>
      CsvToHtmlTable.init({
        csv_path: 'static/val_result.csv', 
        element: 'table-container', 
        allow_download: true,
        csv_options: {separator: ',', delimiter: '"'},
        datatables_options: {
          "paging": false, 
          "order": [[1, 'desc']],
          "columnDefs": [
          {targets: [0], className: 'dt-left', className: 'dt-head-left'},
          ]
        }
      });
    </script>
  
  </body>
  </html>
